# Hill-Climbing Search

Given a problem returns a local maximum state.

From the initial state, the highest-valued successor of the current state is selected and if it is higher than the current state, the current state is updated to the successor and the process is repeated, otherwise the current state is returned.

# Simulated Annealing

Given a problem and a schedule (a function that returns the temperature at a given time) returns a solution state.

From the initial state, the next temperature is computed at the given time, if the temperature is zero, the current state is returned, otherwise a random successor of the current state is selected and if it is higher than the current state, the current state is updated to the successor, otherwise the current state is updated to the successor with a probability of $e^{\deltaE/T}$. The process is repeated incrementing the time from 1 until possibly infinity.

# Genetic Algorithm

Given a population and a fitness function (a function that returns the fitness of a given state) returns an individual.

Two individuals are selected from the population randomly and a child is generated from them; with a small random probability, the child is mutated. The child is added to the new population and the process is repeated until some individual is fit enough or enough time has elapsed. Then the best individual is returned.

# AND-OR Search

Given a problem returns a conditional plan or a failure.

First, the Or-Search is performed, given the initial state, the problem and the path: if the initial state is a goal state, an empty plan is returned, if the state is on the path, a failure is returned. Then, for each possible action in the current state the And-Search plan is performed passing the states resulting from the action, the problem and the path with the current state appended. If the plan is not a failure, the action, follwed by the plan is returned, otherwise a failure is returned.

For the And-Search, given states, a problem and a path: for each state passed, the Or-Search is performed and if the plan is a failure, a failure is returned, otherwise the plan is returned as the plan for the current state.

# Online-DFS-Agent

Given a percept that identifies the current state returns an action, making use of a result table, an untried actions table and an unbacktracked actions table, with s and a the previous state and action, respectively.

If the percept is the goal, return stop; if it is a new state (not in untried), all actions are added to untried of that state; if it exists a previous state, the new state is added to the result from the action of the previous state and also added to the front of unbactracked of the previous state; if there are no untried action for the current state, if also there are no unbacktracked actions, return stop, otherwise, return the first unbacktracked action and remove it from the list; otherwise select the first untried action, remove it from the list and return it, after assigning the new state.

# LTRA*-Agent

Given a percept that identifies the current state returns an action, making use of a result table, an H table for the costs, and s and a the previous state and action, respectively.

If the percept is the goal, return stop; if it is a new state (not in H), the heuristic cost is assigned to the table; if it exists a previous state, the new state is added to the result from the action of the previous state and the cost estimate for the previous state is updated with the one that minimizes the cost of going from the previous state to the new state plus the cost estimate of the new state; the action that minimizes the cost estimate is selected and returned, after assigning the new state.