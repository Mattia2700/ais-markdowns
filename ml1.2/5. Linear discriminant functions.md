# Discriminant learning

There are two types of learning algorithms: generative and discriminative. Generative algorithms estimate the probability distribution of the problem data; discriminative algorithms directly model a function that maps the input data to the output data, to discriminate between the classes. 

- Pros of discriminative algorithms: 
    - If the data are complex, the generative model may be too complex to be useful.
    - If the goal is to discriminate between classes, the data distribution is not needed.
    - Focuses parameters on the desired goal.

- Cons of discriminative algorithms:
    - The model is less flexible.
    - You cannot perform inference on the data.
    - Not possible to generate new data of a given class.

## Linear functions

Linear functions are functions of the form $f(x) = w^Tx + w_0$. It is an hyperplane that splits positive and negative examples using a linear combination of example features, with $w_0$ being a *bias* or *threshold*. Since it is the simplest discriminative model, it can be the first to be tried, but with more complex data it may not be the best choice, if the data is not linearly separable.

A linear binary classifier can be obtained by looking at the sign of the function $f(x)$. If $f(x) > 0$, then the example is classified as positive, otherwise if $f(x) < 0$ it is classified as negative, and if $f(x) = 0$ it is the decision boundary. The weight vector **w** is orthogonal to the decision hyperplane: $$ w^Tx + w_0 - w^Tx' - w_0 = 0 \rarr w^T(x-x') = 0 $$ 

The value $f(x)$ returned for a certain point $x$ is called **functional margin** and can be seen as the confidence in the prediction: the larger the value, the more confident the classifier is in the prediction. The **geometric margin** is the distance between the decision hyperplane and the closest point $x$ of the training set. The geometric margin is the ratio between the functional margin and the norm of the weight vector (projection of the point in the hyperplane plus $r^x$ time the unit vector of weights): $$ r^x = \frac{f(x)}{||w||} = \frac{w^Tx + w_0}{||w||} $$

## Perceptron 

The perceptro architecture is based on the actual neurons of the brain: it takes information from the previous neurons, sums them up and sends a signal to the next neuron according to a threshold. A single perceptron combines the inputs linearly and takes the sign of the result as the output.

While it can solve AND, OR and NOT problems, it cannot solve XOR problems, because it is not possible to separate the two classes with a linear function, and would require a two-layer perceptron: this because the XOR problem can be written in CNF or DNF with two clauses; if more complex functions are required, the multi-layer perceptron can be used, but it will blow up exponentially with the number of layers.

The bias can be incorporated in the weight vector by adding a $w_0$ as the first element of the vector, and a $1$ as the first element of the input vector, obtaining augmented vectors.

Generally speaking, to train a perceptron (learning parameters) you need a function of parameters that can be optimized, like the error on the training set: $$ E(w; D) = \sum_{(x,y) \in D} l(y,f(x)) $$

What you want to do is find the parameters that minimize the error (**possible risk of overfitting training data**), and this is done by gradient descent: $$ w \leftarrow w - \eta \nabla E(w; D) $$ where $\eta$ is the learning rate, too high and you may overshoot the minimum, too low and it will take too long to converge; other approch is to varying it during the training, starting with a high value and decreasing it over time.

This misclassification error is not a good measure because it it piecewise constant and not differentiable, and makes it a poor choice for gradient descent. A better choice is considering only the misclassified examples ($yf(x)\le0$), and sum them up as $-yf(x)$. After calculating the gradient, the update rule becomes: $$ w \leftarrow w + \eta \sum_{(x,y) \in D_E} yx $$

The problem of this update rule is the fact the sum operation becomes expensive if the dataset is large: a possible solution is to use a stochastic gradient descent, where the update rule is applied to a subset of the training set (extreme case: one example at a time), and the update rule becomes: $$ w \leftarrow w + \eta yx $$ 

## Perceptron regression

The perceptron can also be used to output a real value, instead of a class label. With $X$ as the input training matrix and $y$ as the output vector, the perceptron can be seen as a set of linear equations: $ Xw = y$, with solution $ w = X^{-1}y $. Usually, the matrix $X$ is rectangular, with more rows than columns (examples > features), with an overdetermined system of equations and no exact solution. 

Instead, loss minimization is performed using the **mean squared error** (MSE): $$ E(w; D) = \sum_{(x,y) \in D} (y-f(x))^2 = (y-Xw)^T(y-Xw) $$ that has a closed form solution and gradient descent can be used. The closed form solution is $w=(X^TX)^{-1}X^Ty$, with $(X^TX)^{-1}X^T$ being the pseudoinverse of $X$, or the left-inverse of $X$: if X is square and nonsigular, inverse and left-inverse are the same and the result is the exact solution; if the left-inverse is full rank, this means the features are linearly independent (if not, remove redudant ones); otherwise, the update rule is $$w \leftarrow w - \eta \sum_{(x,y) \in D} (y-f(x))(-x_i)$$

## Multiclass classification

The perceptron can be extended to multiclass classification by using a one-vs-all (class x or not, m classifier, training on n pairs) approach: for each class, a binary classifier is trained, and the class with the highest score is chosen. Another possibility is to use a one-vs-one or all-pairs (class x or y, m(m-1)/2 classifier, training on 2n/m pairs) approach, where a binary classifier is trained for each pair of classes, and the class with the highest number of wins is chosen.

Gaussian classifier is a (log) generative linear classifier, because a gaussian distribution is used to model the data, and a linear function is used to classify the data. The same holds for Naive Bayes classifier, with $$ f_i(x) = P(x|y_i)P(y_i) = \prod_{j=1}^{|{x}|} \prod_{k=1}^K \theta_{ky_i}^{z_k(x[j])} \frac{|D_i|}{|D|} = \prod_{k=1}^K \theta_{ky_i}^{N_kx} \frac{|D_i|}{|D|} $$ where $N_kx$ is the number of times the feature $k$ appears in $x$. Log operation can be applied to $f_i(x)$.