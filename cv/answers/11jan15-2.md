# 1. Planar homography

Sure. A planar homography is a transformation that maps points from one plane to another plane. It can be computed using the following steps:

1. Find four corresponding points in the two planes. These points should be well-distributed and not collinear.
2. Form a matrix of the corresponding points, where each row represents a point.
3. Solve the matrix for the homography. This can be done using a least-squares method.
4. Once the homography is computed, it can be used to map points from one plane to another.

Here is an example of how a planar homography can be computed and employed.

* Suppose we have two images of a car, one taken from the front and one taken from the side. We want to find the planar homography that maps points from the front image to the side image.
* We first find four corresponding points in the two images. These points could be the corners of the car's bumper, for example.
* We then form a matrix of the corresponding points, as shown below.

```
[x1 y1 1]
[x2 y2 1]
[x3 y3 1]
[x4 y4 1]
```

* We can solve this matrix for the homography using a least-squares method. The result will be a 3x3 matrix, which is the homography.
* Once the homography is computed, we can use it to map points from the front image to the side image. For example, if we have a point (x, y) in the front image, we can map it to the side image using the following equation:

```
(x', y') = H * (x, y)
```

where H is the homography and (x', y') is the point in the side image.

Planar homographies are used in computer vision for a variety of tasks, such as:

* Object recognition: A planar homography can be used to match features between two images of the same object, even if the object is not in the same position or orientation in the two images.
* Camera calibration: A planar homography can be used to calibrate a camera, which is the process of determining the intrinsic and extrinsic parameters of the camera.
* Image stitching: A planar homography can be used to stitch together two or more images to create a wider image.

# 2. Fundamental matrix

The correct answer is **c. There is no such assumption**. The fundamental matrix is a 3x3 matrix that relates the coordinates of any two images of the same scene by describing the epipolar geometry between them¹²³. The epipolar geometry is the geometric relationship between the two cameras that took the images and the 3D points in the scene¹²³. The fundamental matrix constrains where the projection of a point from the scene can occur in both images, such that for any pair of corresponding points, their coordinates satisfy the equation x'^T F x = 0, where x and x' are the homogeneous coordinates of the points in the first and second image, respectively¹²³.

The fundamental matrix does not assume that the extrinsic or intrinsic parameters of the cameras are known¹²³. The extrinsic parameters are the rotation and translation of the camera with respect to the world coordinate system, and the intrinsic parameters are the focal length and the principal point of the camera². The fundamental matrix can be estimated from at least eight pairs of corresponding points between the two images, without knowing anything about the cameras¹²³. There are different methods to estimate the fundamental matrix, such as the eight-point algorithm, the normalized eight-point algorithm, or the RANSAC algorithm¹²³.

The fundamental matrix is a generalization of another matrix called the essential matrix, which is defined as E = R T K'^T F K, where R and T are the relative rotation and translation between the two cameras, and K and K' are their intrinsic matrices². The essential matrix operates on image points expressed in normalized coordinates, which are obtained by multiplying the image points by the inverse of the intrinsic matrix². The essential matrix assumes that the cameras are calibrated, meaning that their intrinsic parameters are known². The essential matrix can be estimated from at least five pairs of corresponding points between the two images, if their intrinsic parameters are known². (**not sure**)

# 4. Fundamental matrix

The answer is **b. Extrinsic and intrinsic parameters are known**.

The depth of a point can be computed using stereo vision. Stereo vision is a technique that uses two or more cameras to recover the three-dimensional structure of a scene.

In order to compute the depth of a point using stereo vision, the extrinsic and intrinsic parameters of the cameras must be known. The extrinsic parameters are the pose of the cameras, such as their rotation and translation. The intrinsic parameters are the properties of the cameras themselves, such as the focal length and sensor size.

If the extrinsic and intrinsic parameters are not known, then it is not possible to compute the depth of a point using stereo vision.

Option a. is incorrect because it is not necessary for the cameras to be parallel and aligned. In fact, the cameras can be at any angle to each other, as long as the extrinsic and intrinsic parameters are known.

Option c. is incorrect because the intrinsic parameters are not sufficient to compute the depth of a point. The extrinsic parameters are also needed.

# 5. Least squares

The correct answer is **b. Minimize the error between the observed value and the one obtained after applying the transformation**. The camera matrix is a 3x4 matrix that relates the coordinates of a point in the 3D world to its projection on the 2D image plane². The camera matrix can be decomposed into two components: the intrinsic matrix, which contains the focal length and the principal point of the camera, and the extrinsic matrix, which contains the rotation and translation of the camera with respect to the world coordinate system². To compute the camera matrix, we need to have correspondences between 3D world points and 2D image points, which can be obtained by using a calibration object with known geometry, such as a chessboard or a grid¹³. The least squares approach is used to find the camera matrix parameters that minimize the error between the observed image points and the ones obtained after applying the transformation by the camera matrix¹³. This error is also known as the reprojection error¹³.

The least squares approach is not used to minimize the distance between the object and the camera because this distance is not relevant for computing the camera matrix¹²³. The distance between the object and the camera only affects the size of the object projection on the image plane, but not its shape or orientation². The camera matrix can be computed for any distance as long as there are enough correspondences between 3D and 2D points¹³.

The least squares approach is not used to compute the best observation point of an object because this is not a well-defined problem¹²³. The best observation point of an object depends on many factors, such as the lighting conditions, the occlusions, the noise, and the purpose of the observation¹²³. The camera matrix does not capture these factors, but only models the geometric relationship between 3D and 2D points². The camera matrix can be computed for any observation point as long as there are enough correspondences between 3D and 2D points¹³.