# 1. Correspondences in stereo matching

The correct answer is **c. Correlation is computed using a block of pre-defined size**. Correlation is a measure of similarity between two signals or images. In stereo matching, correlation is used to find the best match for a pixel or a block of pixels in one image with another image taken from a different viewpoint. The correlation is computed using a block of pre-defined size, such as 3x3 or 5x5, because using a single pixel (b) is not robust to noise and illumination changes, and using the histogram of the image (a) does not capture the spatial information of the pixels¹².

One method of stereo matching that uses correlation is the **block matching algorithm**. The block matching algorithm works as follows:

- For each pixel or block of pixels in the left image, find a set of candidate pixels or blocks in the right image that are within a certain search range along the same horizontal line (called the epipolar line).
- For each candidate pixel or block in the right image, compute the correlation with the pixel or block in the left image using a similarity metric, such as sum of absolute differences (SAD), sum of squared differences (SSD), or normalized cross-correlation (NCC).
- Select the candidate pixel or block in the right image that has the highest correlation with the pixel or block in the left image as the best match.
- Repeat this process for all pixels or blocks in the left image.
- The disparity between each pair of matched pixels or blocks is then calculated as the difference in their horizontal coordinates. The disparity map can be used to estimate the depth of the scene.

# 2. Camera calibration

The correct answer is **c. Min 6**. Camera calibration is the process of estimating the intrinsic and extrinsic parameters of a camera, such as the focal length, the principal point, the lens distortion, and the rotation and translation between the camera and the world coordinate system. Camera calibration can be done using a known calibration pattern, such as a checkerboard or a circle grid, that has a fixed size and shape¹².

To calibrate a camera, at least six matching pairs of image points and world points are necessary. This is because the camera calibration problem can be formulated as a system of linear equations with 11 unknowns (five intrinsic parameters and six extrinsic parameters). To solve this system, at least 11 equations are needed, which correspond to six matching pairs of image points and world points². However, in practice, more than six matching pairs are usually used to improve the accuracy and robustness of the calibration.

There are different methods to solve the equations for camera calibration, depending on the type of calibration pattern and the assumptions made about the camera model. One popular method is **Zhang's method**, which uses planar grids as calibration patterns²³. Zhang's method works as follows:

- Take several images of a planar grid from different orientations and extract the corners of the grid as image points.
- Assume that the intrinsic parameters of the camera are fixed and do not change across the images.
- For each image, estimate the extrinsic parameters (rotation and translation) of the camera relative to the grid using a homography transformation. A homography transformation is a linear mapping between two planes that preserves collinearity and ratios of distances. It can be represented by a 3x3 matrix H that relates the image points x and the world points X as x = HX³.
- Solve for H using the linear least squares method, which minimizes the sum of squared errors between the observed image points and the predicted image points using H.
- Once H is obtained, extract the extrinsic parameters from its elements using some algebraic manipulations³.
- Repeat this process for all images and obtain a set of extrinsic parameters for each image.
- Use these extrinsic parameters to estimate the intrinsic parameters of the camera using another linear least squares method, which minimizes the sum of squared errors between the observed image points and the predicted image points using both intrinsic and extrinsic parameters³.
- Refine the estimated intrinsic and extrinsic parameters using a nonlinear optimization method, such as Levenberg-Marquardt, which minimizes the sum of squared errors between the observed image points and the predicted image points using a more realistic camera model that accounts for lens distortion³.

# 3. SIFT features

The answer is **b. Local features that evaluate the gradient information**.

SIFT features are local features that are invariant to scale, rotation, and illumination. They are extracted by first finding keypoints in an image, which are points that are locally distinctive. The keypoints are then described using a descriptor, which is a vector that summarizes the gradient information around the keypoint.

The gradient information is the change in intensity between neighboring pixels. It is a measure of the edges and textures in an image. SIFT features are invariant to scale because they are based on the gradient information, which is scale-invariant. They are also invariant to rotation and illumination because they are normalized to remove the effects of these factors.

SIFT features are widely used in computer vision applications, such as image matching, object recognition, and 3D reconstruction.

Here are some of the other properties of SIFT features:

* They are local, which means that they are only sensitive to a small region around the keypoint. This makes them robust to noise and occlusions.
* They are distinctive, which means that they can be uniquely identified in different images. This makes them useful for object recognition.
* They are repeatable, which means that they can be extracted from the same keypoint in different images. This makes them useful for 3D reconstruction.

The correct answer is **b. Local features that evaluate the gradient information**. SIFT (Scale-Invariant Feature Transform) is a feature extraction method that reduces the image content to a set of points (called keypoints or interest points) that are used to detect similar patterns in other images¹⁴. SIFT features are local because they are based on the appearance of the object at particular interest points, and not on the whole image². SIFT features are invariant to image scale and rotation, meaning that they can be recognized even if the object is resized or rotated²⁵. SIFT features are also robust to changes in illumination, noise, and minor changes in viewpoint².

SIFT features evaluate the gradient information because they use the orientation and magnitude of the image gradients to describe the keypoints. Image gradients are the changes in pixel intensity values across the image, and they indicate the edges and corners of the objects. SIFT features use a histogram of oriented gradients (HOG) to capture the distribution of the gradients within a local region around each keypoint²⁵. The HOG descriptor is then normalized to reduce the effect of illumination changes².

# 4. Viola-Jones face detection

The correct answer is **a. Binary 2D functions**. The Viola-Jones algorithm is a machine-learning technique for object detection, especially face detection, that uses a cascade of simple features to quickly and efficiently locate the objects of interest in an image²⁴. The simple features are called **Haar-like features**, which are binary 2D functions that evaluate the difference in pixel intensity values between adjacent rectangular regions within a detection window¹³⁵. For example, a Haar-like feature can be a horizontal edge, a vertical edge, or a center-surround region. The value of a Haar-like feature is computed by subtracting the sum of the pixels in the white region from the sum of the pixels in the black region³.

The Viola-Jones algorithm uses an efficient data structure called **integral image** to compute the Haar-like features in constant time, regardless of the size of the feature or the detection window²³⁵. An integral image is an image where each pixel value is the sum of all the pixel values above and to the left of it in the original image³.

The Viola-Jones algorithm also uses a machine-learning technique called **AdaBoost** to select the most relevant and discriminative Haar-like features from a large pool of possible features and to combine them into a strong classifier that can distinguish between faces and non-faces²³⁵. AdaBoost works by iteratively training weak classifiers (based on single features) on weighted subsets of the training data and then assigning them weights according to their classification errors³.

The Viola-Jones algorithm further uses a **cascade** of classifiers to speed up the detection process and to reject non-face regions as early as possible²³⁵. A cascade is a sequence of classifiers, each consisting of a different number of features, arranged from simple to complex. The cascade works by applying the first classifier (with few features) to every sub-window in the image and discarding those that do not pass as potential faces. Then, it applies the second classifier (with more features) to the remaining sub-windows and discards those that do not pass as potential faces. This process continues until all classifiers are applied or all sub-windows are rejected³.

# 5. Planar homography

The correct answer is **c. compute the correspondences between two planes when they are not parallel**. A planar homography is a bijective linear mapping of pixels between two images of the same planar surface in space⁴⁵. A planar homography can be represented by a 3x3 matrix that transforms the homogeneous coordinates of a point in one image to the homogeneous coordinates of the corresponding point in another image⁵. A planar homography is useful to compute the correspondences between two planes when they are not parallel because it can account for the perspective distortion caused by the different viewpoints⁴⁵. For example, a planar homography can be used to warp an image of a book cover to match the perspective of another image of the same book cover taken from a different angle⁵.

A planar homography is not useful to understand the affine transformation of an object because an affine transformation is a simpler type of mapping that preserves parallelism, ratios of distances, and collinearity, but not angles or lengths⁴⁵. An affine transformation can be represented by a 2x3 matrix that transforms the Cartesian coordinates of a point in one image to the Cartesian coordinates of the corresponding point in another image⁵. A planar homography is more general than an affine transformation and can model more complex transformations that involve perspective⁴⁵.

A planar homography is not useful to compute the height of an object in millimeters because it does not provide any information about the absolute scale or depth of the scene⁴⁵. A planar homography only relates the relative positions and orientations of pixels between two images, but it does not tell us how far away the object is from the camera or how big it is in reality⁴⁵. To compute the height of an object in millimeters, we would need additional information such as the focal length of the camera, the distance between the camera and the object, or the size of a reference object in the scene⁵.