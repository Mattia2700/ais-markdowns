# 3. GMM vs adaptive background subtraction

Sure. Here are the advantages of a Gaussian Mixture Model (GMM) for background subtraction compared to adaptive background subtraction:

* **Robust to illumination changes.** GMM can model the background as a mixture of Gaussian distributions, which makes it more robust to changes in illumination than adaptive background subtraction methods, which typically model the background as a single Gaussian distribution.
* **Handles multiple object classes.** GMM can be used to model multiple object classes, which is useful for applications such as video surveillance, where there may be a variety of objects in the scene. Adaptive background subtraction methods typically only model a single object class, which can lead to false positives.
* **Less sensitive to noise.** GMM is less sensitive to noise than adaptive background subtraction methods. This is because GMM models the background as a distribution of points, rather than a single point, which helps to smooth out noise.

Here are some additional advantages of GMM:

* It can be used to model non-stationary backgrounds.
* It can be used to track objects over time.
* It can be used to detect moving objects in a scene.

However, GMM also has some disadvantages, such as:

* It can be computationally expensive to train and update the model.
* It can be sensitive to the choice of parameters.
* It can be difficult to model complex backgrounds.

Overall, GMM is a powerful and versatile method for background subtraction. It is particularly well-suited for applications where the background is non-stationary or where there are multiple object classes.

Adaptive background subtraction is a simpler method that is less computationally expensive than GMM. However, it is less robust to changes in illumination and noise, and it can be more sensitive to the choice of parameters.

The best method for background subtraction will depend on the specific application. If the background is relatively static and there is little noise, then adaptive background subtraction may be a good choice. However, if the background is dynamic or there is a lot of noise, then GMM may be a better option.

# 4. Stereo vision order

The correct procedure for computing the depth of a point using a stereo system is:

1. Camera calibration. This involves determining the intrinsic and extrinsic parameters of the cameras.
2. Computation of correspondences. This involves finding the corresponding points in the two images of the stereo pair.
3. Disparity estimation. This involves estimating the disparity between the corresponding points.
4. Depth estimation. This involves estimating the depth of the point from the disparity.

Therefore, the answer is **(b)**.

The following is a brief explanation of each step:

* **Camera calibration** is necessary to correct for the distortions introduced by the camera lenses. This is done by taking a set of images of a known calibration pattern and then using a computer algorithm to estimate the intrinsic and extrinsic parameters of the cameras.
* **Computation of correspondences** is the process of finding the corresponding points in the two images of the stereo pair. This is done by matching features between the two images. Features can be points, edges, or regions.
* **Disparity estimation** is the process of estimating the disparity between the corresponding points. Disparity is the difference in the distance between the two cameras of the stereo vision system.
* **Depth estimation** is the process of estimating the depth of the point from the disparity. This is done by using the known baseline of the stereo vision system and the disparity.

The procedure described above is the most common approach for computing the depth of a point using a stereo system. However, there are other approaches that can be used. For example, some approaches use deep learning to automate the process of matching features and estimating disparity.

# 5. SIFT Hessian matrix

The Hessian matrix is computed **(b)** to check if the candidate point is strong enough.

The SIFT algorithm consists of two main steps:

1. **Keypoint detection**. This step identifies points in the image that are likely to be interest points.
2. **Descriptor extraction**. This step extracts a descriptor for each interest point that describes its local appearance.

The Hessian matrix is used in the keypoint detection step to determine if a point is a local extremum. A local extremum is a point where the image intensity changes rapidly in all directions. The Hessian matrix is a 2x2 matrix that measures the second-order partial derivatives of the image intensity at a point. The determinant of the Hessian matrix is used to determine if the point is a local maximum, local minimum, or saddle point.

The Hessian matrix is computed right after the computation of the DoG. The DoG is a difference of Gaussians, which is a way to smooth an image while preserving edges and corners. The Hessian matrix is computed for each scale and orientation of the DoG image.

The candidate points that are identified as local extrema are then further processed to remove those that are not strong enough. This is done by checking the eigenvalues of the Hessian matrix. If the eigenvalues are too small, then the point is not considered to be strong enough and is discarded.

The remaining points are then used to extract descriptors. The descriptors are used to match features between images and to identify objects in images.

So the answer is (b).