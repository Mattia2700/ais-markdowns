# AI Act 

## Definitions

An AI is a software that is capable of producing outputs such as content, predictions, recommendations, or decisions influencing the environment they interact with; they are developed using ML approaches (e.g. supervised, unsupervised, reinforcement, deep learning), logic and knowledge-based approaches (inductive programming, KB, inference and deduction engines and expert systems), or statistical approaches (e.g. Bayesian estimation, optimization methods).  

## Risk-based approach / Prohibited systems / Prohibited artificial intelligence practices 

The AI Act is a risk-based approach to AI regulation. There are four different risk levels, each with a different set of requirements. The risk levels are defined by the following criteria:

- Unacceptable risk (prohibited): like social scoring, facial recognition, dark-pattern AI or manipulation

    - The application that violates fundamental rights by subliminal techniques, manipulation, social scoring or real-time remote biometric identification systems in public spaces

- **High risk (conformity assessment): AI with application in education, employment, justice, immigration or law**

    - AI systems to be used as safety component of a product or a product itself with application in:

        - Critical infrastructure (e.g. transport)
        - Biometric ID systems
        - Educational and vocational training (e.g. automated scoring of exams)
        - Employment, workers management and access to self-employment (automated hiring or CV triage software)
        - Essential private and public services (e.g. automated welfare benefits systems)
        - Law enforcement systems that may with peopleâ€™s fundamental rights ('pre-crime' detection)
        -  Migration, asylum and border management (e.g.verification of authenticity of travel documents)
        - Administration of justice and democratic processes (e.g. automated sentencing assistance)

- Limited risk (transparency): like chatbots, deep fakes or emotion recognition systems

    - Providers have obbligation of transparency letting users know they are interacting with a machine rather than a human

- Minimal risk (code of conduct): like spam filters or video games

## Requirements for high-risk AI systems

High-risk AI systems shall comply with the following requirements:

- Risk management system:
    - an iterative process running throughout its entire lifecycle, consisting of identification and analysis of risks, estimation and evaluation of risks
    - risk shall be eliminated or reduced as much as possible, with mitigation measures in place if risks cannot be eliminated or reduced
    - user shall be trained to use the system safely, with technical knowledge about it
    - testing shall be performed
    - specific consideration shall be taken into account especially when children are supposed to use the system

- Data and data governance:
    - training, validation and testing data shall managed correctly, especially speaking about relevant design, data preparation and assumptions, examinating possible biases and assuring representativeness and completeness of the data

- Technical documentation:
    - documentation about the AI system shall be created before it is placed in the market

- Record keeping:
    - the AI system shall be able to keep records of its decisions and actions (including periods, databases, input data)

- Transparency and provision of information to users:
    - AI systems operations shall be transparent to users, enabling them to understand the system output and using it safely, eventually accompanied by the instructions for use provided by the manufacturer
    - instructions shall contain intended purpose of the system, level of accuracy, robustness and cybersecurity, any circumstances which may lead to risks, performance information, and eventual specification for input data
    - human oversight and expected lifetime (with eventual maintenance and measures to ensure correct functioning) shall be provided

- Human oversight:
    - appropriate human-machine interface tools shall be provided to enable human oversight of the AI system, to prevent or minimize risks
    - the humans responsible of oversigthing the AI system shall fully understand the system and its limitations in order to monitor its operation, as well as being able to correctly interpret the system output, being able to override or reverse the system output when necessary, and being able to interrupt the system operation when necessary (with a "stop" button)

- Accuracy, robustness and cybersecurity:
    - Relevant accuracy measures shall be provided
    - Redundancy shall be ensured to prevent error or inconsistencies
    - Prevent unauthorized access by third parties to alter use or performance by exploiting vulnerabilities; also, data poisoning and adversarial examples shall be prevented

## Classification of AI systems as High-Risk

An AI system is classified as high-risk if it is a product or a component of a product placed on the market and used by third parties. When deciding whether an AI system may cause harm to the health, the following aspects shall be considered:

- the intended purpose of the system
- if the system already caused harm to the health and safety of people and its extent
- if the outcome of the system is easily reversible

## Transparency obligations

People shall be informed about the fact that they are interacting with an AI system, unless it is obvious. This applies to emotion recognition systems and biometric categorization systems (unless authorized by the law), and when osserving output of deep fakes.

## Obligations of providers of high-risk AI systems / Conformity assessment and standards

Providers of high-risk AI systems shall:

- have a quality management system in place
- draw up the technical documentation
- keep the logs automatically generated
- ensure the AI system undergoes the relevant conformity assessment procedure
- comply with the registration obligations
- take the necessary corrective actions
- inform the national competent authorities about the AI system
- affix the CE marking to the AI system
- demonstrate the conformity to the requirements

## Obligations of users of high-risk AI systems

Users of high-risk AI systems shall:

- use the AI system in accordance with the instructions for use
- monitor the AI system operation used in accordance with the instructions for use; if this use may cause harm to the health, the user shall inform the provider of the AI system about it, including incidents or malfunctions if any, and stop using the AI system
- keep automatically generated logs

## Regulatory sandboxes

- AI regulatory sandboxes shall provide a controlled environment that facilitate the development, testing and validation of AI systems before they are placed on the market. Here, the processing of personal data shall fall under the regulations.

- Any significant risk shall be followed by an immediate mitigation, and if failing, the development and test shall be stopped.

- Member States shall coordinate their activities in according with the framework of the European AI Board.

## Critical issues


