# AI Act 

## Definitions

An AI is a software that is capable of producing outputs such as content, predictions, recommendations, or decisions influencing the environment they interact with; they are developed using ML approaches (e.g. supervised, unsupervised, reinforcement, deep learning), logic and knowledge-based approaches (inductive programming, KB, inference and deduction engines and expert systems), or statistical approaches (e.g. Bayesian estimation, optimization methods).  

## Risk-based approach / Prohibited systems / Prohibited artificial intelligence practices 

The AI Act is a risk-based approach to AI regulation. There are four different risk levels, each with a different set of requirements. The risk levels are defined by the following criteria:

- Unacceptable risk (prohibited): like social scoring, facial recognition, dark-pattern AI or manipulation

    - The application that violates fundamental rights by subliminal techniques, manipulation, social scoring or real-time remote biometric identification systems in public spaces

- **High risk (conformity assessment): AI with application in education, employment, justice, immigration or law**

    - AI systems to be used as safety component of a product or a product itself with application in:

        - Critical infrastructure (e.g. transport)
        - Biometric ID systems
        - Educational and vocational training (e.g. automated scoring of exams)
        - Employment, workers management and access to self-employment (automated hiring or CV triage software)
        - Essential private and public services (e.g. automated welfare benefits systems)
        - Law enforcement systems that may with peopleâ€™s fundamental rights ('pre-crime' detection)
        -  Migration, asylum and border management (e.g.verification of authenticity of travel documents)
        - Administration of justice and democratic processes (e.g. automated sentencing assistance)

- Limited risk (transparency): like chatbots, deep fakes or emotion recognition systems

    - Providers have obbligation of transparency letting users know they are interacting with a machine rather than a human

- Minimal risk (code of conduct): like spam filters or video games

## Requirements for high-risk AI systems

High-risk AI systems should comply with the following requirements:

- Risk management system:
    - an iterative process running throughout its entire lifecycle, consisting of identification and analysis of risks, estimation and evaluation of risks
    - risk should be eliminated or reduced as much as possible, with mitigation measures in place if risks cannot be eliminated or reduced
    - user should be trained to use the system safely, with technical knowledge about it
    - testing should be performed
    - specific consideration should be taken into account especially when children are supposed to use the system

- Data and data governance:
    - training, validation and testing data should managed correctly, especially speaking about relevant design, data preparation and assumptions, examinating possible biases and assuring representativeness and completeness of the data

- Technical documentation:
    - documentation about the AI system should be created before it is placed in the market

- Record keeping:
    - the AI system should be able to keep records of its decisions and actions (including periods, databases, input data)

- Transparency and provision of information to users:
    - AI systems operations should be transparent to users, enabling them to understand the system output and using it safely, eventually accompanied by the instructions for use provided by the manufacturer
    - instructions should contain intended purpose of the system, level of accuracy, robustness and cybersecurity, any circumstances which may lead to risks, performance information, and eventual specification for input data
    - human oversight and expected lifetime (with eventual maintenance and measures to ensure correct functioning) should be provided

- Human oversight:
    - appropriate human-machine interface tools should be provided to enable human oversight of the AI system, to prevent or minimize risks
    - the humans responsible of oversigthing the AI system should fully understand the system and its limitations in order to monitor its operation, as well as being able to correctly interpret the system output, being able to override or reverse the system output when necessary, and being able to interrupt the system operation when necessary (with a "stop" button)

- Accuracy, robustness and cybersecurity:
    - Relevant accuracy measures should be provided
    - Redundancy should be ensured to prevent error or inconsistencies
    - Prevent unauthorized access by third parties to alter use or performance by exploiting vulnerabilities; also, data poisoning and adversarial examples should be prevented

## Classification of AI systems as High-Risk


