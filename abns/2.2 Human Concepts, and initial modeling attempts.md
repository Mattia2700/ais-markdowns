# Human Concepts, and initial modeling attempts

## Conceptual Structure for AI Students

### Words and Concepts

#### Relations Between Words and Concepts
- **Typical Approach**: Words are associated with concepts or a network of conceptual representations.
- **Objective**: Develop a theory of conceptual structure to understand how words represent meaning.
- **Complications**: Words do not always map to single concepts due to polysemy (multiple meanings for one word).

#### Evidence Against 1:1 Form-Meaning Mapping
- **Polysemy**: A word like "cinema" or "bank" has multiple meanings based on context.
- **Murphy's View**: A single concept cannot fully capture the meaning of a polysemous word.
- **Storage of Senses**: Klein and Murphy (2001) found that different senses of polysemous words can be stored, as seen through priming effects.
- **Context Refinement**: Words may specify a set of potentials refined by context.

#### Words Are Not Concepts
- **Anomia**: A type of aphasia where one cannot retrieve the lexical form of a concept despite recognizing or defining it (e.g., naming a saw).

### Focusing on Conceptual Structure
- **Understanding Word Meaning**: Requires understanding the organization of conceptual structure in the mind.
- **Concepts**: Represent our knowledge and help us identify, infer features, and interact with things.
- **Access Methods**: Words, pictures, or music.
- **Study Focus**: Conceptual structure or "semantic memory" should be studied independently of word meaning.

### The Classic View of Word Meaning
- **Definition**: Sets of necessary and sufficient conditions (e.g., "Bachelor = Man, unmarried").
- **Category Membership**: Objects either belong to a category or do not, with all members being equivalent.
- **Advantages**: Supports hierarchical structure and inheritance.

### Representing the Meaning of Concepts

#### Propositional Networks
- **Example**: Dog is defined within a taxonomy (canine, mammal, vertebrate, etc.).
- **Collins & Quillian (1969)**: Measured response latencies to statements to study conceptual structure, proposing that some features are directly stored while others are inferred.

#### Problems with Propositional Networks
1. **Verification Time**: Influenced by statement frequency, not just hierarchical relations.
2. **ISA Links**: People verify "dogs are animals" faster than "dogs are mammals," challenging hierarchical assumptions.
3. **Logical Inferences**: Assumptions like "if A is a B and B is a C, then A is a C" are not always valid.

### Rosch and the Refutation of the Classic View
- **Insight**: Categories are based on family resemblance, not necessary and sufficient conditions.
- **Prototypical Members**: Highly similar within a category, less similar to other categories.
- **Empirical Support**: Typicality effects in verification times and ratings, showing central and peripheral members in categories.

### Psychological Properties of Prototypes
- **Prototypes**: Hypothetical members with default features.
- **Studies**: Showed correlation between shared features and typicality ratings.

### Fuzzy Category Boundaries
- **Natural Categories**: Do not have fixed boundaries.
- **Mcloskey and Glucksberg (1978)**: Showed varying judgments about category membership and changes over time.

### Levels of Abstraction and Informativeness

#### Example: Identifying a Cat
- **Basic Level**: "Cat" - provides useful information and good discrimination.
- **Subordinate Level**: "Siamese Cat" - more specific but not always necessary.
- **Superordinate Level**: "Animal" - too general, offers little information.

#### Levels of Abstraction
1. **Basic Level**: Useful for general identification.
2. **Subordinate Level**: More specific, used for finer distinctions.
3. **Superordinate Level**: Very general, useful for broader categories.

#### Utility
- **Murphy's Criteria**:
  - **Informativeness**: Amount of facts linked to the category.
  - **Distinctiveness**: Extent to which a category differs from others.
  - **Basic Level**: Balances informativeness and distinctiveness effectively.

This summarized content provides a comprehensive overview of conceptual structure, word meaning, and categorization principles, useful for understanding and applying these concepts in AI and cognitive science.

## Modeling Typicality in Conceptual Organization

### Overview

#### Key Topics
1. Modeling typicality (Lake et al.).
2. Feature models explaining behavioral and brain responses (Mitchell et al., Wen et al., Baroni et al.).
3. Modeling similarity spaces in human behavior and brain responses (Kriegeskorte et al., Peterson et al.).

### Deep Neural Networks and Typicality Ratings

#### Background
- **Objective**: Assess if deep-learning systems can serve as cognitive models by predicting human typicality ratings for natural images.
- **Importance**: Typicality influences various cognitive tasks, such as categorization speed, learning ease, and inductive inference.

#### Convolutional Networks (ConvNets) and Typicality
- **Learning Approach**: ConvNets learn to categorize, potentially by identifying prototypes.
- **Question**: Do ConvNets' representations align with human-perceived typicality?

### Methods

#### Behavioral Method
- **Participants**: Used Mechanical Turk to gather ratings.
- **Task**: Participants rated how well images fit their idea of the category.
- **Categories**: Eight image categories including banana, bathtub, coffee mug, envelope, pillow, soap dispenser, table lamp, and teapot.
- **Reliability**: High human reliability with a split-half correlation of 0.92.

#### Computational Method
- **Architectures**: Three ConvNet architectures including OverFeat (7 layers) with a final 1000-way softmax layer.
- **Performance**: Achieved a top-five error rate of 14.2%, indicating high accuracy.

### Estimating Image Typicality

#### Assumptions
- **Typicality**: Assumed to be related to the model's classification response strength for the category.

#### Methods
1. **Raw Typicality**: 
   - Based on the raw category score.
   - Identifies the abstract representation that maximizes activity for the category.
2. **Contrast Typicality**: 
   - Emphasizes images that strongly load on the correct category over others.
   - Measures how differentiated the category response is from responses to other categories.

#### Results
- Both raw and contrast scores performed similarly well in predicting typicality.

### Interim Conclusion
- Deep ConvNets can learn graded categories and predict human typicality ratings for some everyday categories.

#### Examination Within Layers
- **Activation Vectors**: Computed average activation vectors for 1300 training images per class to serve as category prototypes.
- **Typicality Modeling**: Measured as cosine distance between the activation vector for a new image and the stored prototype.
- **Findings**: Better prediction accuracy was observed in deeper ConvNet layers.

### Summary
Deep neural networks, specifically ConvNets, show potential in modeling human conceptual organization by predicting typicality ratings. Their layered architecture and learned representations align well with human judgments of typicality, suggesting their utility as cognitive models for understanding categorization and typicality in the human brain.

