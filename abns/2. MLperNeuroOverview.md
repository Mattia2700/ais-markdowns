# ML and Neurocognitive Science

## Barrett et al.

In ML recently we assisted to the creaction of Deep Neural Network (DNNs), with millions of parameters, without handcrafted features and high performance, and in some way this can be similar to human brain: 
- first because in both cases there are an high number of interconnected elements (neurons), 
- both transform input in output (stimuli to complex response thanks to multiple processing stages)
- they both analyze high dimensional data.

### Analogies

- Receptive fields:
    - neurons in the visual cortex are specialized to process stimuli in specific spacial areas, with small receptive fields in the beginning and increasing in size in higher level areas of visual processing
        - receptive fields sensitivity increases in higher level areas, where also invariance to small transformations (can detect identity of object but not appearance) can be found
    - Using activation maximization, a field analysis method, we are able to synthesize images that maximally activate a neuron, and we can see the produced images are increasingly complex in higher level areas
- Ablation:
    - it consists in removing a part of the network and see how the performance changes, but it is difficult to do in humans, so we can use brain lesions to have an insight
    - in DNNs it is possible to silencing neurons and see their impact on the output, using structural pruning (removing neuron and outgoing weights)
        - network trained for generalization are more robust than those trained on memorizing labels (importance of neuron is measured by the sum of its effect, not class selectivity)
        - pruning and fine tuning active areas of research
- Dimensionality reduction
    - the brain codes information in a distributed way, necessitating multivariate analysis
        - multiple neurons encode the same information (2 neurons may fire almost identically)
        - information is encoded in a distributed manner (4 classes among 2 neurons)
        - correlation among units indicates that the activity can be reduced to a lower dimensional space
    - in DNNs an object-by-feature matrix from fully connected layer can be compressed by more than 80% almost all variance (few low dimensions explain differences between images)
- Representational geometries:
    - given 2 layers or 2 networks you can compare them using Canonical Correlation Analysis or PLScorrelation
        - they identify lower-level features that capture and maximize correlation/covariance between datasets
    - using Representational Similarity Analysis object-by-object similarity matrix can be constructed
    - neurobiological activation vectors can be predicted from DNN embeddings using linear regression 

