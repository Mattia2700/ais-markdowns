# ML and Neurocognitive Science

## Barrett et al.

In ML recently we assisted to the creaction of Deep Neural Network (DNNs), with millions of parameters, without handcrafted features and high performance, and in some way this can be similar to human brain: 
- first because in both cases there are an high number of interconnected elements (neurons), 
- both transform input in output (stimuli to complex response thanks to multiple processing stages)
- they both analyze high dimensional data.

### Analogies

- Receptive fields:
    - neurons in the visual cortex are specialized to process stimuli in specific spacial areas, with small receptive fields in the beginning and increasing in size in higher level areas of visual processing
        - receptive fields sensitivity increases in higher level areas, where also invariance to small transformations (can detect identity of object but not appearance) can be found
    - Using activation maximization, a field analysis method, we are able to synthesize images that maximally activate a neuron, and we can see the produced images are increasingly complex in higher level areas
- Ablation:
    - it consists in removing a part of the network and see how the performance changes, but it is difficult to do in humans, so we can use brain lesions to have an insight
    - in DNNs it is possible to silencing neurons and see their impact on the output, using structural pruning (removing neuron and outgoing weights)
        - network trained for generalization are more robust than those trained on memorizing labels (importance of neuron is measured by the sum of its effect, not class selectivity)
        - pruning and fine tuning active areas of research
- Dimensionality reduction
    - the brain codes information in a distributed way, necessitating multivariate analysis
        - multiple neurons encode the same information (2 neurons may fire almost identically)
        - information is encoded in a distributed manner (4 classes among 2 neurons)
        - correlation among units indicates that the activity can be reduced to a lower dimensional space
    - in DNNs an object-by-feature matrix from fully connected layer can be compressed by more than 80% almost all variance (few low dimensions explain differences between images)
- Representational geometries:
    - given 2 layers or 2 networks you can compare them using Canonical Correlation Analysis or PLScorrelation
        - they identify lower-level features that capture and maximize correlation/covariance between datasets
    - using Representational Similarity Analysis object-by-object similarity matrix can be constructed
    - neurobiological activation vectors can be predicted from DNN embeddings using linear regression

## Spicer and Sanborn

### Spatial methods

They consist of placing items in a multidimensional space and making use of their location to make conclusion about categorization. This can be done using location relative an hyperplane (perceptron/svm) or by computing its similarity to different prototypes or exemplars (means/centroid):
- in the case of prototypes, the similarity is computed using the mean of a category
- in the case of exemplars, the similarity is computed as a ratio between the similarity of the item to the category and the similarity of the item to the other categories (fit per class)
- using clustering algorithms, the items are grouped into clusters using distance within and between clusters, that can be either hard (one-to-one) or soft (fuzzy)

An example is the Generalized Context Model (GCM):

![Alt text](<images/Screenshot from 2023-06-19 10-35-42.png>)

### Logical methods and ANNs

- logical methods define items using logical statements concering the features of the target, identifying common elements within a dataset that can be defined using a set of rules: you search for rules (either probabilistic) that maximizes discrimination across stimuli
    - its adventage is that it is easy to interpret, and multiple rules can be combined to form complex rules, but creates hard boundaries
- with ANNs you don't make assumption, but directly implement the model, and while they become more complex, they better model human behaviour

## Conclusion

Asking what is the most accurate model is not the right question, because it depends on the task and you should focus on ones offering useful explorations of the ways in which human learning operates. Also, you have to consider if want to be able to understand the underlying representations, so you don't use just accuracy, but what confusions occur.