# Reinforcement Learning

Typically used for robots, games or sequential scheduling problems in general, the idea of reinforcement learning is that the learner can be seen as an agent that needs to take actions depending on its own state and the environment where it acts. For each state $s$, there is a set of actions $A$ that the agent can take. The agent receives a reward $r$ from the environment after taking an action $a$ in a state $s$. The agent's goal is to maximize the total reward it receives over time.

The task is to learn a policy ($Ï€: S \rightarrow A$) that assigns the best action $a$ in each state $s$ to maximize the overall reward (including future moves): the agent, thus, has to learn how to deal with delayed rewards coming from future actions, and how to balance the trade-off between exploitation (taking the best action in the current state) and exploration (taking a random action to discover new states).

## Markov Decision Process (MDP)

To formalize the problem, we can use the Markov Decision Process (MDP) framework. An MDP is a tuple $(S, S_G, A, P, R)$ where:

- $S$ is the set of states the agent can be in
- $S_G$ is the set of terminal states $S_G \subset S$ (possibly empty)
- $A$ is the set of actions the agent can take
- $P$ is the transition model describing the probability of transitioning to state $s'$ from state $s$ taking action $a$ ($P(s'|s,a)$)
- $R$ is the reward function that gives the reward $r$ for taking action $a$ in state $s$ reaching state $s'$ ($R(s,a,s')$)

Since there are immediate and delayed rewards, we need to find a way to calculate utilities over time, which means they are defined over **environment histories** (sequence of states), there are no constraints on the number of steps (**infinite horizon**), and if one history is preferred over another at a given time, it will be preferred in the future as well (**stationary** preferences). There are two ways for defining utilities:

- **Additive rewards**: $U([s_0, s_1, s_2, ...]) = R(s_0) + R(s_1) + R(s_2) + ...$

- **Discounted rewards**: $U([s_0, s_1, s_2, ...]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + ...$

where $\gamma$ is the discount factor, which is a number between 0 and 1. The discount factor is used to balance the trade-off between immediate and delayed rewards. If $\gamma = 0$, the agent will only care about immediate rewards, while if $\gamma = 1$, it will care about delayed rewards as much as immediate ones.

In order to take decisions, we have to find an optimal policy $\pi^*$, describing what action to take in each state, that maximizes the expected utility of the agent, which is defined as the utility of an environment history, taken in expectation over all possible histories generated with that policy (because of non-deteterminism of the transition model). Remember that optimal policies changes when rewards vary, so also choosing the reward function is important.

The utility of a state given a policy $\pi$ is defined as: $$U^\pi(s) = E_\pi[\sum_{k=0}^\infin\gamma^tR(S_{t+k+1})|S_t=s]$$

where $E_\pi$ is the expectation taken with respect to the policy $\pi$ and $S_{t+k+1}$ is the state reached after $k$ steps from state $S_t$ using policy $\pi$, with the true utility being the one under the optimal policy $\pi^*$.

So, when deciding the action to take in a state $s$, we have to choose the action that maximizes the utility of the next state $s'$, which is defined as: $$\pi^*(s) = \argmax_{a \in A} \sum_{s' \in S}P(s'|s,a)U^\pi(s')$$

The problem is that the true utility of a state is utility under the optimal policy, while the optimal policy is defined in terms of the true utility of the state, a chicken-and-egg problem. We can solve this problem by using **Bellman's equation**:

$$U(s) = R(s) + \gamma \max_{a \in A} \sum_{s' \in S}P(s'|s,a)U(s')$$

which means the utility of a state is its immediate reward plus the expected discounted utility of the next state, assuming that the agent chooses and optimal action.

Calculating all the utilities of all the states is quite difficult because of non-linear constraints, so we can use an iterative approach called **value/utility iteration**:

- Initialize $U(s) = 0$ for all $s \in S$
- Repeat until max utility difference is less than a threshold:
    - Bellman update each state $s$: $U_{i+1}(s) = R(s) + \gamma \max_{a \in A} \sum_{s' \in S}P(s'|s,a)U_i(s')$
    - Update $i \leftarrow i+1$

Another iterative approach is **policy iteration**, where you iteratively improve the policy instead of the utilities, from a random one to the optimal one:

- Initialize $\pi_0$ randomly
- Repeat until no policy improvement:
    - Policy evaluation: calculate utilities $U_i$ using Bellman's equation $$U_i(s) = R(s) + \gamma \sum_{s' \in S}p(s'|s,\pi_i(s))U_{i}(s') \space \forall s \in S$$ ($\pi_i(s)$ is the action taken in state $s$ by policy $\pi_i$)
    - Policy improvement: $$\pi_{i+1}(s) = \argmax_{a \in A} \sum_{s' \in S}p(s'|s,a)U_i(s') \space \forall s \in S$$
    - Update $i \leftarrow i+1$

## Partial Knowledge

In the previous section, we assumed that the agent has full knowledge of the environment, but this is not always the case. In fact, the agent can only observe the state $s$ and the reward $r$ after taking an action $a$ in a state $s$, and it can only observe the next state $s'$ after taking an action $a$ in a state $s$.

Here the utilities are learned by space exploration, and based on what is learned, we have two different approaches:

### POLICY EVALUATION

Here policy is given, environment is learned (**passive agent**). 

An algorithm is **Adaptive Dynamic Programming (ADP)**):
- Loop until reached state is terminal:
    - Receive reward $r$ for current state $s$ (initially chosen randomly) and set $R(s)=r$
    - Choose action $a$ based on policy $\pi(s)$
    - Take action $a$, reach state $s'$
    - Update counts of state-action pair $N_{sa}$ and next-state-given-state-action pair $N_{s'|sa}$
    - Update transition model $P(s'|s,a) = N_{s'|sa}/N_{sa}$
    - Update utility estimate

It performs ML estimation of transition probabilities and then update the utility estimate each step, resulting in a **expensive** algorithm.

To avoid this, an approximative strategy is used, called **Temporal-difference (TD)**, in order to not perform policy evaluation at each step for all states (only the reached ones). It is based on the fact that if transition from $s$ to $s'$ is observed and $s'$ is always the successor of $s$, the utility of s should be $U(s) = R(s) + \gamma U(s')$, so you define it as: $$U(s) \leftarrow U(s) + \alpha(R(s) + \gamma U(s') - U(s))$$ where $\alpha$ is the learning rate.

The TD algorithm is:
- Loop until reached state is terminal:
    - Receive reward $r$ for current state $s$ (initially chosen randomly) 
    - Choose action $a$ based on policy $\pi(s)$
    - Take action $a$, reach state $s'$
    - Update utility estimate $U(s) \leftarrow U(s) + \alpha(R(s) + \gamma U(s') - U(s))$

Here you don't need to update the transition model every time, each step is much faster, but takes longer to converge to the optimal policy, making it same as ADP on the long run.

### POLICY IMPROVEMENT

Here both policy and environment are learned (**active agent**). One option is to take ADP and then replace the step that does policy evaluation with policy computation using Bellman's equation, and then evaluate the policy, but the learned policy could be suboptimal (optimal about what I learned, not what I did not), since the knowledge of the environment is incomplete.

An exploitation-exploitation tradeoff should be considered, following promising directions and exploring new ones. 

An $\epsilon$-greedy policy can be used, where with probability $\epsilon$ the agent chooses a random action, otherwise would be greedy (choosing the action that maximizes the utility of the next state). Another approach is assigning higher utilities to unexplored state-action pairs, so that the agent explores them more often ($U^+(s)=R(s)+\gamma \max_{a \in A} f(\sum_{s' \in S}P(s'|s,a)U^+(s'), N_a)$, with f increasing for the first parameter and decreasing for the second one).

Also here, TD can be used, in order to learn an action utility function $Q(s,a)$, combining the utility of the states with the utility of the actions, with the optimal policy being $\pi^*(s) = \argmax_{a \in A} Q(s,a)$,

The algorithm, called **SARSA** (on-policy TD learning), explores new states and their rewords, compute the utility of the state-action and uses it to define its updated policy ($\epsilon$-greedy):

- Loop until reached state is terminal:
    - Receive reward $r$ for current state $s$ (initially chosen randomly) 
    - Choose action $a$ based on policy $\pi^\epsilon(s)$ (based on $Q$)
    - Take action $a$, reach state $s'$
    - Choose action $a'$ based on policy $\pi^\epsilon(s')$ to update local utility estimate (based on $Q$)
    - Update local utility estimate $Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma Q(s',a') - Q(s,a))$

Another algorithm, called **Q-learning** (off-policy TD learning), instead of choosing $a'$ based on the current policy, it chooses the one that maximes the current state-action utility:

- Loop until reached state is terminal:
    - Receive reward $r$ for current state $s$ (initially chosen randomly) 
    - Choose action $a$ based on policy $\pi^\epsilon(s)$ (based on $Q$)
    - Take action $a$, reach state $s'$
    - Update action utility estimate $Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma \max_{a' \in A} Q(s',a') - Q(s,a))$

In one case, Q is updated using the **current policy**'s action, whereas in the other case it is updated using the **greedy policy**'s action (different from the one used to search for the next state): this last method is more flexible, because learn from traces generated by unknown policies, while the other one tends to converge faster.

## Large State Spaces

All previous techniques assume a tabular representation of the state space, but in many cases the state space is too large to be represented in a table (e.g. Backgammon has $10^{20}$ states). In this case, we can use **function approximation** to represent the state space, using a function $f$ that maps state vectors to a utility estimate $U(s) = f(s)$, using parameters learned in the process. This is helpful to generalize to unseen states ($\phi$).

Speaking about action utility function approximation, it can be done by looking at the state-action pairs using a tabular representation (**Q Learning**), or by modeling a neural network that takes as input the state and outputs the utility of each action (**Deep Q Learning**).

To learn state utility function, the TD error is calculated as $$E(s,s')=\frac{1}{2}(R(s)+\gamma U_\theta(s')-U_\theta(s))^2$$ with gradient $$\nabla_\theta E(s,s') = (R(s)+\gamma U_\theta(s')-U_\theta(s))(-\nabla_\theta U_\theta(s))$$ and stochastic gradient update $$\theta=\theta-\alpha \nabla_\theta E(s,s')=\theta+\alpha(R(s)+\gamma U_\theta(s')-U_\theta(s))(\nabla_\theta U_\theta(s))$$

To learn action utility function, the TD error is calculated as $$E((s,a),s')=\frac{1}{2}(R(s)+\gamma\max_{a' \in A}Q_\theta(s',a')-Q_\theta(s,a))^2$$ with gradient $$\nabla_\theta E((s,a),s') = (R(s)+\gamma\max_{a' \in A}Q_\theta(s',a')-Q_\theta(s,a))(-\nabla_\theta Q_\theta(s,a))$$ and stochastic gradient update $$\theta=\theta-\alpha \nabla_\theta E((s,a),s')=\theta+\alpha(R(s)+\gamma\max_{a' \in A}Q_\theta(s',a')-Q_\theta(s,a))(\nabla_\theta Q_\theta(s,a))$$