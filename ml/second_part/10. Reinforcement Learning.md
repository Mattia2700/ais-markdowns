# Reinforcement Learning

Typically used for robots, games or sequential scheduling problems in general, the idea of reinforcement learning is that the learner can be seen as an agent that needs to take actions depending on its own state and the environment where it acts. For each state $s$, there is a set of actions $A$ that the agent can take. The agent receives a reward $r$ from the environment after taking an action $a$ in a state $s$. The agent's goal is to maximize the total reward it receives over time.

The task is to learn a policy ($Ï€: S \rightarrow A$) that assigns the best action $a$ in each state $s$ to maximize the overall reward (including future moves): the agent, thus, has to learn how to deal with delayed rewards coming from future actions, and how to balance the trade-off between exploitation (taking the best action in the current state) and exploration (taking a random action to discover new states).

## Markov Decision Process (MDP)

To formalize the problem, we can use the Markov Decision Process (MDP) framework. An MDP is a tuple $(S, S_G, A, P, R)$ where:

- $S$ is the set of states the agent can be in
- $S_G$ is the set of terminal states $S_G \subset S$ (possibly empty)
- $A$ is the set of actions the agent can take
- $P$ is the transition model describing the probability of transitioning to state $s'$ from state $s$ taking action $a$ ($P(s'|s,a)$)
- $R$ is the reward function that gives the reward $r$ for taking action $a$ in state $s$ reaching state $s'$ ($R(s,a,s')$)

Since there are immediate and delayed rewards, we need to find a way to calculate utilities over time, which means they are defined over **environment histories** (sequence of states), there are no constraints on the number of steps (**infinite horizon**), and if one history is preferred over another at a given time, it will be preferred in the future as well (**stationary** preferences). There are two ways for defining utilities:

- **Additive rewards**: $U([s_0, s_1, s_2, ...]) = R(s_0) + R(s_1) + R(s_2) + ...$

- **Discounted rewards**: $U([s_0, s_1, s_2, ...]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + ...$

where $\gamma$ is the discount factor, which is a number between 0 and 1. The discount factor is used to balance the trade-off between immediate and delayed rewards. If $\gamma = 0$, the agent will only care about immediate rewards, while if $\gamma = 1$, it will care about delayed rewards as much as immediate ones.

In order to take decisions, we have to find an optimal policy $\pi^*$, describing what action to take in each state, that maximizes the expected utility of the agent, which is defined as the utility of an environment history, taken in expectation over all possible histories generated with that policy (because of non-deteterminism of the transition model). Remember that optimal policies changes when rewards vary, so also choosing the reward function is important.

The utility of a state given a policy $\pi$ is defined as: $$U^\pi(s) = E_\pi[\sum_{k=0}^\infin\gamma^tR(S_{t+k+1})|S_t=s]$$

where $E_\pi$ is the expectation taken with respect to the policy $\pi$ and $S_{t+k+1}$ is the state reached after $k$ steps from state $S_t$ using policy $\pi$, with the true utility being the one under the optimal policy $\pi^*$.

So, when deciding the action to take in a state $s$, we have to choose the action that maximizes the utility of the next state $s'$, which is defined as: $$\pi^*(s) = \argmax_{a \in A} \sum_{s' \in S}P(s'|s,a)U^\pi(s')$$

The problem is that the true utility of a state is utility under the optimal policy, while the optimal policy is defined in terms of the true utility of the state, a chicken-and-egg problem. We can solve this problem by using **Bellman's equation**:

$$U(s) = R(s) + \gamma \max_{a \in A} \sum_{s' \in S}P(s'|s,a)U(s')$$

which means the utility of a state is its immediate reward plus the expected discounted utility of the next state, assuming that the agent chooses and optimal action.

Calculating all the utilities of all the states is quite because of non-linear constraints, so we can use an iterative approach called **value/utility iteration**:

- Initialize $U(s) = 0$ for all $s \in S$
- Repeat until max utility difference is less than a threshold:
    - Bellman update each state $s$: $U_{i+1}(s) = R(s) + \gamma \max_{a \in A} \sum_{s' \in S}P(s'|s,a)U_i(s')$
    - Update $i \leftarrow i+1$

Another iterative approach is **policy iteration**, where you iteratively improve the policy instead of the utilities, from a random one to the optimal one:

- Initialize $\pi_0$ randomly
- Repeat until no policy improvement:
    - Policy evaluation: calculate utilities $U_i$ using Bellman's equation $$U_i(s) = R(s) + \gamma \sum_{s' \in S}p(s'|s,\pi_i(s))U_{i}(s') \space \forall s \in S$$ ($\pi_i(s)$ is the action taken in state $s$ by policy $\pi_i$)
    - Policy improvement: $$\pi_{i+1}(s) = \argmax_{a \in A} \sum_{s' \in S}p(s'|s,a)U_i(s') \space \forall s \in S$$
    - Update $i \leftarrow i+1$

## Partial Knowledge

In the previous section, we assumed that the agent has full knowledge of the environment, but this is not always the case. In fact, the agent can only observe the state $s$ and the reward $r$ after taking an action $a$ in a state $s$, and it can only observe the next state $s'$ after taking an action $a$ in a state $s$.

Here the utilities are learned by space exploration, and based on what is learned, we have two different approaches:

### POLICY EVALUATION

Here policy is given, environment is learned (**passive agent**). 

An algorithm is **Adaptive Dynamic Programming (ADP)**):
- Loop until reached state is terminal:
    - Receive reward $r$ for current state $s$ (initially chosen randomly) and set $R(s)=r$
    - Choose action $a$ based on policy $\pi(s)$
    - Take action $a$, reach state $s'$
    - Update counts of state-action pair $N_{sa}$ and next-state-given-state-action pair $N_{s'|sa}$
    - Update transition model $P(s'|s,a) = N_{s'|sa}/N_{sa}$
    - Update utility estimate

It performs ML estimation of transition probabilities and then update the utility estimate each step, resulting in a **expensive** algorithm.

To avoid this, an approximative strategy is used, called **Temporal-difference (TD)**, in order to not perform policy evaluation at each step for all states (only the reached ones). It is based on the fact that if transition from $s$ to $s'$ is observed and $s'$ is always the successor of $s$, the utility of s should be $U(s) = R(s) + \gamma U(s')$, so you define it as: $$U(s) \leftarrow U(s) + \alpha(R(s) + \gamma U(s') - U(s))$$ where $\alpha$ is the learning rate.

The TD algorithm is:
- Loop until reached state is terminal:
    - Receive reward $r$ for current state $s$ (initially chosen randomly) 
    - Choose action $a$ based on policy $\pi(s)$
    - Take action $a$, reach state $s'$
    - Update utility estimate $U(s) \leftarrow U(s) + \alpha(R(s) + \gamma U(s') - U(s))$

Here you don't need to update the transition model every time, each step is much faster, but takes longer to converge to the optimal policy, making it same as ADP on the long run.


### POLICY IMPROVEMENT

Here Both policy and environment are learned (**active agent**).