# Unsupervised Learning
 
We have seen supervised learning, where we have a set of labeled data and we try to learn a function that maps the input to the output. In unsupervised learning, we do not have labeled data. Instead, we have a set of data points and we try to find some structure in the data, like **clusters**.

## k-means clustering

k-means clustering is an unsupervised learning algorithm that tries to find k clusters in the data, with each cluster represented by its mean $\mu_i$. The algorithm is as follows:

1. Initialize cluster means $\mu_1, \mu_2, \dots, \mu_k$ randomly.
2. Repeat until convergence:
    1. Assign each example to the cluster with the closest mean.
    2. Update cluster means to be the average of the examples in the cluster.

To measure the distance between two points, we can use:

- Euclidean distance: $\sqrt{\sum_{i=1}^d (x_i - x'_i)^2}$
- Generic Minkowski distance ($p\ge1$): $(\sum_{i=1}^d |x_i - x'_i|^p)^{1/p}$
- Cosine similarity: $\frac{x^T \cdot x'}{\|x\| \|x'\|}$
- Malahanobis distance: $\sqrt{(x - x')^T \cdot \Sigma^{-1} \cdot (x - x')}$

To inspect the quality of the clustering, we can use the sum-of-squared errors (SSE): $\sum_{i=1}^k \sum_{x \in D_i} \|x - \mu_i\|^2$ with $\mu_i$ the sample mean of the cluster $D_i$. It is the sum of the squared distances between each example and its cluster mean and it is minimized when the clusters are tight and well-separated.

## Gaussian Mixture Model (GMM)

Again in GMM you fix the number of clusters a priori, but each cluster is represented by a Gaussian distribution. What I have to estimate are the mean and covariance of each Gaussian, using the EM algorithm, repeated until convergence:

1. Compute expected cluster assignment given the current parameters $\theta$
2. Maximize parameters $\theta$ given cluster assignment

To be able to maximize the parameters, we have to introduce **latent variables** $z_i$ that indicate which cluster each example belongs to ($z_{ij}=1$ if Gaussian $j$ generated example $i$, $0$ otherwise). The EM algorithm is as follows, in the of k **univariate** Gaussians:

1. Initialize parameters $\theta$ randomly.
2. Iterate until parameters difference is below a threshold:
    1. **E-step**: compute $$\mathbb{E}[z_{ij}]=\frac{p(x_i|\mu_j)}{\sum_{l=1}^k p(x_i|\mu_l)} = \frac{exp(-\frac{1}{2\sigma^2}(x_i-\mu_j)^2)}{\sum_{l=1}^k exp(-\frac{1}{2\sigma^2}(x_i-\mu_l)^2)}$$ of each latent variable given $\theta$.
    2. **M-step**: calculate new parameters $$\mu_i=\frac{\sum_{i=1}^n \mathbb{E}[z_{ij}]x_i}{\sum_{i=1}^n \mathbb{E}[z_{ij}]}$$
     assuming values of latent variables are their expected values.

# **??DEMONSTRATION??**

But, how to choose number of clusters? The more clusters, the more homogeneous the data will be inside the class (smaller error). You have to trade-off quality with quantity, stopping increasing the number of clusters when the error does not decrease significantly anymore.

First of all, if you use the **elbow method** you run the clustering algorithm for increasing number of clusters and then you plot each cluster's SSE. You can see that the error generally decreases as the number of clusters increases, and there is one point where the error stops decreasing significantly and starts to flatten out. This is the **elbow** of the curve, and it is the optimal number of clusters.

The problem with this method is that it can be ambiguous with multiple candidates for the elbow. Instead, you can use the **average silhouette method** to measure the quality of the clustering; this works because the more the number of clusters, the more each cluster is homogeneous internally, but the more different clusters start to become more similar: you want to trade-off intra-cluster similarity with inter-cluster dissimilarity. To choose the best number of clusters, you plot the average silhouette for increasing number of clusters and you choose the one with the highest value.

To calculate it, you compute the average distance between the example and the examples in its own cluster ($a_i=d(i,C)=\frac{1}{|C|}\sum_{j \in C} d(i,j)$, then the same but with the closest cluster ($b_i=\min_{C' \neq C} d(i,C')$). Then you compute the silhouette of each example as $$s_i=\frac{b_i-a_i}{\max(a_i,b_i)}$$ and the average silhouette of the clustering is the average of the silhouette of each example.

## Hierarchical clustering

Hierarchical clustering is a clustering algorithm that builds a hierarchy of clusters, like natural grouping of data. There are two types of hierarchical clustering, both greedy:

- **Top-down**: start from a single cluster with all examples and then recursively split clusters into subclusters

- **Bottom-up**: start with n clusters of individual examples (singletons) and then recursively merge closest pairs of clusters (**agglomerative** hierarchical clustering)

What you get is a **dendrogram** that represents the hierarchy of clusters. You can cut the dendrogram at any point to get the desired number of clusters.

Instead of using a similarity measure between clusters, you can use an evaluation criterion.

## Similarity measures

To measure the similarity between two clusters, you can use:

- **Nearest neighbor**: the similarity between two clusters is the difference between the closest pair of points in the two clusters
- **Farthest neighbor**: the similarity between two clusters is the difference between the farthest pair of points in the two clusters
- **Average distance**: the similarity between two clusters is the average distance between all pairs of points in the two clusters
- **Distance between means**: the similarity between two clusters is the distance between the means of the two clusters

The first two can be instable because they are more sensitive to outliers. The average one can be expensive if the cluster are large. The last one is more efficient, you only deal with the means of the clusters.