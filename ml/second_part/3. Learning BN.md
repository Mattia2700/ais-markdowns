# Learning BN

In order to learn a BN we need to know the conditional probability distribution of each variable, and we can use the data to estimate them, that acts like examples of the actual distribution, and can contribute to some or all the varaibles.

The easiest way to learn a BN is to use the **Maximum Likelihood Estimation (MLE)**, which is the method that maximizes the probability of the data given the model. In this case, the model is the BN, and the data is the dataset.

$$\theta^{max}=argmax_{\theta} p(D|\theta) = L(D,\theta)$$

where $\theta$ is the set of parameters of the model, and $D$ is the dataset.

In case of complete data: $$p(D|\theta) = \prod_{i=1}^N p(x(i)|\theta)=\prod_{i=1}^N \prod_{j=1}^m p(x_j(i)|pa_j(i), \theta) = \prod_{i=1}^N \prod_{j=1}^m p(x_j(i)|pa_j(i), \theta_{X_j|pa_j})$$

where $x_j(i)$ is the $j$-th variable of the $i$-th sample, and $pa_j(i)$ is the set of parents of $x_j (i)$.

In this way, maximazing over theta over different tables, you can consider them as separate problems, and you can use the MLE for each table: this means you can compute each likelihood independently, considering only the relevant variables of the graph, reducing the complexity of the problem (for example $P(BusLate|sunny,cloudy)$ can be decomposed in $P(BusLate|sunny)$ and $P(BusLate|cloudy)$)

After dividing the problem in smaller ones, the actual likelihood is the product of the likelihoods of each table: $$L(\theta_{X|Pa}, D)=\prod_{i=1}^N p(x(i)|pa(i), \theta_{X|Pa}) = \prod_{i=1}^N \theta_{x(i)|u(i)} = \prod_{u \in Val(U)} \prod_{x \in Val(X)} \theta_{x|u}^{N_{u,x}}$$

where $N_{u,x}$ is the number of times that the value $x$ of $X$ appears in the dataset, given that the value of $U$ is $u$ (multinomial distribution).

After you set the gradient to zero, your best parameters are: $$\theta_{x|u}^{max} = \frac{N_{u,x}}{\sum_{x} N_{u,x}} = \frac{N_{u,x}}{N_u}$$

But, if some data is missing, you can't say that the probability of that value is zero (maybe you don't have that example), so you need to use the **Maximum a Posteriori (MAP)**: in this case, instead of maximazing the likelihood, you maximize the likelihood times the prior, which is the probability of the model given the data (except for the normalization factor, Bayes theorem). 

When modeling the prior, it is chosen to be part of the same family of the likelihood, so that it is a conjugate prior. In the case of the multinomial distribution, the MAP estimate is: $$ \theta_{x|u}^{max} = \frac{N_{u,x} + \alpha_{x|u}}{\sum_{x} N_{u,x} + \alpha_{x|u}} $$

In this case, the parameters $\alpha_{x|u}$ (parameter of Dirichlet distribution, the prior), count as virtual counts, and help to avoid the problem of zero probabilities and the problem of overfitting.

## Incomplete Data

Sometimes, the examples are not complete, you know something about the variables, but not everything, like the way they are related (partial knowledge). In this case, you can use the **Expectation Maximization (EM)** algorithm, which is an iterative algorithm that alternates between two steps: the **E-step**, which computes the expected value of the (log-)likelihood, and the **M-step**, which maximizes the expected value of the log-likelihood.

- e-step: $$\mathbb{E}_{p(X|D, \theta)}[N_{ijk}] = \sum_{l=1}^n p(x_i(l)=x_k, Pa_i(l)=pa_j|X_l, \theta)$$

    - Basically, you compute the probability that node $i$ in example $l$ takes value $x_k$ and the parents of node $i$ in example $l$ take value $j$, given $x_l$ and the current version of the parameters. Of course if variable $x_i$ and all its parents are fully observed, then the probability is either 1 or 0. Then you sum over all the examples.

- m-step: $$\theta_{ijk}^* = \argmax_{\theta} p(D_c|\theta) = \frac{\mathbb{E}_{p(X|D, \theta)}[N_{ijk}] (+ \alpha_{ijk})}{\sum_{k=1}^{r_i}\mathbb{E}_{p(X|D, \theta)}[N_{ijk}] (+ \alpha_{ijk})} $$

You take the expected count of the likelihood and you divide it by the sum of the expected counts over all possible values $k$ (values of the node), in order to normalize column-wise ($P(BusLate|rainy)$ or $P(BusLate|cloudy)$ or $P(BusLate|sunny)$ are all elements of the same row, with each column representing evidence). Eventually, you add the virtual counts $\alpha_{ijk}$.

## Approaches

There are different approaches to learn a BN:

- **constraint-based**: you want to test conditional independences on the data and you construct a model satisfying them
- **score-based**: you assign a score to each possible structure (varying the nodes and the arcs) and then you search among them to find the best one
- **model-based**: a prior is added to each possible structure, and the best one is chosen according to the posterior probability